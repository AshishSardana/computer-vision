install.packages("swirl")
install.packages("swirl")
install.packages("swirl")
library("swir")
library("swirl")
exit()
install.packages("e1071")
install.packages("rpart")
install.packages("igraph")
install.packages("nnet","randomforest")
install.packages("nnet")
library("swirl")
swirl()
bye()
install_from_swirl("Statistical Inference")
bye()
source('~/EXL Data Analytics /exl_r.R')
source('~/EXL Data Analytics /exl_r.R')
source('~/EXL Data Analytics /exl_r.R')
source('~/EXL Data Analytics /exl_r.R')
source('~/EXL_Data_Analytics /exl_r.R')
source('~/EXL_Data_Analytics /exl_r.R')
source('~/EXL_Data_Analytics /exl_r.R')
source('~/EXL_Data_Analytics /exl_r.R')
source('~/EXL_Data_Analytics /exl_r.R')
source('~/EXL_Data_Analytics /exl_r.R')
source('~/EXL_Data_Analytics /exl_r.R')
source('~/EXL_Data_Analytics /exl_r.R')
source('~/EXL_Data_Analytics /exl_r.R')
source('~/EXL_Data_Analytics /exl_r.R')
source('~/EXL_Data_Analytics /exl_r.R')
source('~/EXL_Data_Analytics /exl_r.R')
source('~/EXL_Data_Analytics /exl_r.R')
getwd()
source('~/EXL_Data_Analytics /exl_r.R')
xvec <- rnorm(200)
yvec <- 2.5*xvec + rnorm(length(xvec))
plot(xvec, yvec)
model1 <- lm(formula = yvec ~ xvec)
summary(model1)
model2 <- lm(xvec ~ yvec)
summary(model1)
summary(model2)
cor(xvec,yvec)^2
model1$coefficients[2]*model2$coefficients[2]
model1 <- lm(formula = yvec ~ xvec -1)
model2 <- lm(xvec ~ yvec -1)
model1$coefficients[2]*model2$coefficients[2]
model1$coefficients[1]*model2$coefficients[1]
install.packages("pcor")
install.packages("ppcor")
library("ppcor")
cars
load("cars")
load(cars)
head(CARS)
head(cars)
iris
data <- iris[-ncol(iris)]
data
sigmax <- cov(data)
sx <- qr.solve(data)
sx <- qr.solve(sigmax)
pcx <- cov2cor(sx)
pcx
pcor(sx)
pc(data)
pcor(data)
pcx <- cov2cor(sx)
pcx
pcor(data)
pnorm(25, mean = 27.3, sd = 2.2, lower.tail = FALSE)
pnorm(30, mean = 27.3, sd = 2.2, lower.tail = T)
set.seed(5)
?rnorm
m_norm <- matrix(rnorm(25),5,5)
m_unif <- matrix(runif(25),5,5)
set.seed(5)
m_unif <- matrix(runif(25),5,5)
set.seed(5)
m_norm <- matrix(rnorm(25),5,5)
m_norm%*%m_unif
m <- m_norm%*%m_unif
m[3,4]
c <- rnorm + i*runif
Re(C) <- m_norm
?Re
C <- complex(real = m_norm, imaginary = m_unif)
c
c[1]
c[1,2]
C <- matrix(complex(real = m_norm, imaginary = m_unif),5,5)
C
C[1,4]
Mod(C[1,4])
Arg(C[1,4])
k <- qr.solve(m_norm)
k <- t(m_norm)
qr.solve(m_norm)
t(m_norm)
k <- qr.solve(m_norm)
det(k)
library(readr)
dataset <- read_csv(NULL)
View(dataset)
library(readr)
w1_q8 <- read_csv("~/Eighth sem/Time Series/Assignment/w1_q8.Rdata")
View(w1_q8)
data1 <- load("/home/ashu/Eighth sem/Time Series/Assignment/w1_q8.Rdata")
mean(data1)
str(data)
str(data1)
data1 <- as.matrix(load("/home/ashu/Eighth sem/Time Series/Assignment/w1_q8.Rdata"))
data1
load("/home/ashu/Eighth sem/Time Series/Assignment/w1_q8.Rdata")
data1 <- load("/home/ashu/Eighth sem/Time Series/Assignment/w1_q8.Rdata")
load("/home/Downloads/w1_q8.Rdata")
load("/home/ashu/Downloads/w1_q8.Rdata")
x <- load("/home/ashu/Downloads/w1_q8.Rdata")
x
x
x
x
x
x
x
x
x
x
x
xx
x
x
xxxx
x
x
x
x
as.data.frame(x)
load("/home/ashu/Downloads/w1_q8.Rdata")
mean(data)
sd(data)
var(data)
load("co2")
co2
d <- co2
head(co2)
head(d)
head(d)
head(co2)
str(CO2)
summary(CO2)
summary(co2)
load("/home/ashu/Eighth sem/Time Series/Assignment/w1_q10.Rdata")
plot(airpass)
plot(airpass,plot.type = 'r')
plot(airpass,'ro')
plot(airpass)
data
co2
str(co2)
dim(co2)
plot(co2)
data1 <- as.ts(co2)
str(data1)
data1 <- as.data.frame(co2)
str(data1)
head(data1)
co2["Jan"]
co2[1]
co2[100]
co2["1997"]
co2[1997]
?ts
t <- co2
d <- as.matrix(t)
d <- matrix(d,ncol = 12)
d
d <- as.matrix(t)
d <- as.vector(t)
d <- matrix(d,ncol = 12)
d
colMeans(d)
which(colMeans(d) == max(colMeans(d)))
?apply
apply(d, 2, var)
l <- apply(d, 2, var)
which(l = max(l))
which(l == max(l))
co2
d <- as.data.frame(t(matrix(co2, 12)))
d
which(colMeans(d) == max(colMeans(d)))
l <- apply(d, 2, var)
which(l == max(l))
data1 <- load("/home/ashu/Eighth sem/Time Series/Assignment/w1_q10.Rdata")
load("/home/ashu/Eighth sem/Time Series/Assignment/w1_q10.Rdata")
demo()
help()
help(cor, verbose = T)
print("Hello World")
help(sum)
?sum
plot.ts(co2)
?help
#Ashutosh
56 <- y
67 <- y
67 -> y
x = 3 + 1i*4
x
log10(1)
print(10)
x = 2.3456
print(x , digits = 3)
c(1,2,3,4,5,6)
xvec <- c(2,5,3,4,6,8)
length(x <- c(1,2,3))
matrix(data = )
matrix(data = 100, nrow = 4, ncol = 25)
array(c(1:3),c(2,3))
array(1,c(2,2,2))
d <- array(1,c(2,2,2))
d[1]
d[1][1]
d[1][1][1]
d[1][1][1][1]
d[1,1,1]
d[1,1,1,1]
d[1,1,1]
dim(d)
d[,,1]
plot(x,y)
x = rnorm(100)
y = rnorm(100)
plot(x,y)
plot(x)
plot(x,y)
plot(x,y, type = 'l')
is.ts(x)
plot(x,type='l')
plot(x,type='b')
plot(x,type='h')
sort(x)
plot(sort(x),y,type='h')
plot(sort(x),y,type='l')
so2
install.packages("astsa")
library("astsa")
?sample
z <- rnorm(200)
v <- rnorm(200)
w <- rnorm(200)
library("ppcor")
x <- 2*z + 3*v
y <- z + w
spcor(x,y)
?spcor
spcor(data.frame(x,y)
)
spcor(data.frame(x,y,z)
)
m <- array(c(1.4471, 0.1756, 0.0891), c(0.1756, 1.1556, 0.1835), c(0.0891, 0.1835, 1.1545))
m
m <- array(c(1.4471, 0.1756, 0.0891), c(0.1756, 1.1556, 0.1835), c(0.0891, 0.1835, 1.1545))
m <- as.matrix(m)
m
m[1]
?array
array(c(1:3),c(2,3))
array()
m <- array(c(1.4471, 0.1756, 0.0891), c(0.1756, 1.1556, 0.1835), c(0.0891, 0.1835, 1.154))
m
m <- c(1.4471, 0.1756, 0.0891, 0.1756, 1.1556, 0.1835,0.0891, 0.1835, 1.154)
matrix(data = m,byrow = T,nrow = 3)
m <- matrix(data = m,byrow = T,nrow = 3)
cor(m)
cov2cor(m)
load("/home/ashu/Eighth sem/Time Series/Assignment/w2_q8.Rdata")
load("/home/ashu/Eighth sem/Time Series/Assignment/w2_q8.Rdata")
load("/home/ashu/Eighth sem/Time Series/Assignment/w2_q8.Rdata")
load("/home/ashu/Eighth sem/Time Series/Assignment/w2_q8.Rdata")
load("/home/ashu/Eighth sem/Time Series/Assignment/w2_q8.Rdata")
load("/home/ashu/Eighth sem/Time Series/Assignment/w2_q8.Rdata")
load("/home/ashu/Eighth sem/Time Series/Assignment/w2_q8.Rdata")
load("/home/ashu/Eighth sem/Time Series/Assignment/w2_q8.Rdata")
load("/home/ashu/Eighth sem/Time Series/Assignment/w2_q8.Rdata")
load("/home/ashu/Eighth sem/Time Series/Assignment/")
load("/home/ashu/Eighth sem/Time Series/Assignment/w2_q8.Rdata")
load("/home/ashu/Eighth sem/Time Series/Assignment/w2_q8.RData")
load("/home/ashu/Eighth sem/Time Series/Assignment/w2_q8.RData")
load("/home/ashu/Eighth sem/Time Series/Assignment/w2_q8.RData")
load("/home/ashu/Eighth sem/Time Series/Assignment/w2_q8.RData")
load("/home/ashu/Eighth sem/Time Series/Assignment/w2_q8.RData")
load("/home/ashu/Eighth sem/Time Series/Assignment/w2_q8.RData")
cor(x,y)
cor(x,y)^2
d <- EuStockMarkets
summary(d)
library("ppcor")
d
summary(d)
str(d)
d <- matrix(d, byrow = F, ncol = 4)
d <- as.data.frame(d)
cor(d$V1, d$V3)
pcor(d$V1, d$V3)
pcor(d)
x <- rnorm(200)
z <- rnorm(200)
v <- rnorm(200)
w <- rnorm(200)
x <- 2*z + 3*v
y <- z + w
pcor(data.frame(x,y,z))
spcor(data.frame(x,y,z))
load("/home/ashu/Downloads/w3_q7.Rdata")
acf(xk,lag.max = 12)
iris
help("iris")
load(iris)
dat <- iris
summary(dat)
y <- dat$Sepal.Length; x <- dat$Sepal.Width
linmod = y~x
linmod = lm(y~x)
linmod$coefficients
plot(x,y)
summary(linmod)
?power.t.test
library(readr)
pkgs <- read_csv("~/Downloads/pkgs.csv")
View(pkgs)
check <- function(x){if( !(x %in% rownames(installed.packages())) ) {install.packages(x)}}
check(pkgs$Package)
apply(pkgs$Package , check)
lapply(pkgs$Package , check)
install.packages(x)
install.packages(x)
install.packages(x)
install.packages(x)
install.packages(x)
install.packages(x)
rm(list = ls())
library(text2vec)
library(data.table)
library(stringr)
path <- "/home/ashu/Hack2Innovate/Samsung_sms_classification"
setwd(path)
train <- read.csv("TRAIN_SMS.csv" , stringsAsFactors = FALSE)
test <- read.csv("DEV_SMS.csv" , stringsAsFactors = FALSE)
sample_sub <- read.csv("sample_submission.csv" , stringsAsFactors = FALSE)
train_lbl <- train$Label
test_ID <- test$RecordNo
n_test <- nrow(test)
n_train <- nrow(train)
train_sms <- train$Message
test_sms <- test$Message
sms <- c(train_sms, test_sms)
sms <- data.frame(sms , stringsAsFactors = FALSE)
sms$id <- 1:nrow(sms)
sms$sms <- apply(data.frame(sms$sms , stringsAsFactors = FALSE), 1, function(x) str_replace_all(x, "[[:punct:]]", " ") )
word_count <- sapply(gregexpr("[[:alpha:]]+", sms$sms), function(x) sum(x > 0))
setDT(sms)
setkey(sms, id)
set.seed(2017L)
all_ids = sms$id
# define preprocessing function and tokenization function
prep_fun = tolower
tok_fun = word_tokenizer
stop_words = c("a", "the")
# Creating tokens
it_train = itoken(sms$sms,
preprocessor = prep_fun,
tokenizer = tok_fun,
ids = sms$id,
progressbar = TRUE)
vocab = create_vocabulary(it_train , stopwords = stop_words , ngram = c(1L, 2L))
# Pruning vocabulory
pruned_vocab = prune_vocabulary(vocab,
term_count_min = 10,
doc_proportion_max = 0.5,
doc_proportion_min = 0.001)
vectorizer = vocab_vectorizer(pruned_vocab)
dtm_train = create_dtm(it_train, vectorizer)
## Text frequency Inverse Document Frequency
tfidf = TfIdf$new()
# fit model to train data and transform train data with fitted model
dtm_train_tfidf = fit_transform(dtm_train, tfidf)
dim(dtm_train_tfidf)
rm(list = ls())
library(text2vec)
library(data.table)
library(stringr)
path <- "/home/ashu/Hack2Innovate/Samsung_sms_classification"
setwd(path)
train <- read.csv("TRAIN_SMS.csv" , stringsAsFactors = FALSE)
test <- read.csv("DEV_SMS.csv" , stringsAsFactors = FALSE)
sample_sub <- read.csv("sample_submission.csv" , stringsAsFactors = FALSE)
train_lbl <- train$Label
test_ID <- test$RecordNo
n_test <- nrow(test)
n_train <- nrow(train)
train_sms <- train$Message
test_sms <- test$Message
sms <- c(train_sms, test_sms)
sms <- data.frame(sms , stringsAsFactors = FALSE)
sms$id <- 1:nrow(sms)
sms$sms <- apply(data.frame(sms$sms , stringsAsFactors = FALSE), 1, function(x) str_replace_all(x, "[[:punct:]]", " ") )
word_count <- sapply(gregexpr("[[:alpha:]]+", sms$sms), function(x) sum(x > 0))
setDT(sms)
setkey(sms, id)
set.seed(2017L)
all_ids = sms$id
# define preprocessing function and tokenization function
prep_fun = tolower
tok_fun = word_tokenizer
stop_words = c("a", "the")
# Creating tokens
it_train = itoken(sms$sms,
preprocessor = prep_fun,
tokenizer = tok_fun,
ids = sms$id,
progressbar = TRUE)
vocab = create_vocabulary(it_train , stopwords = stop_words , ngram = c(1L, 2L))
# Pruning vocabulory
pruned_vocab = prune_vocabulary(vocab, doc_proportion_min = 0.01)
vectorizer = vocab_vectorizer(pruned_vocab)
dtm_train = create_dtm(it_train, vectorizer)
## Text frequency Inverse Document Frequency
tfidf = TfIdf$new()
# fit model to train data and transform train data with fitted model
dtm_train_tfidf = fit_transform(dtm_train, tfidf)
dim(dtm_train_tfidf)
fin_data <- as.matrix(dtm_train_tfidf)
fin_data2 <- as.data.frame(fin_data)
fin_data2$word_count <- word_count
tr <- fin_data2[1:n_train,]
te <- fin_data2[n_train+1:ncol(fin_data2), ]
tr$label <- train_lbl
te$RecordNo <- test_ID
length(test_ID)
length(te)
dim(fin_data2)
n_train
te <- fin_data2[n_train+1:ncol(fin_data2), ]
dim(te)
dim(fin_data2)
te <- fin_data2[n_train+1:nrow(fin_data2), ]
tr$label <- train_lbl
te$RecordNo <- test_ID
source('~/Hack2Innovate/Samsung_sms_classification/preprocessing.r')
dim(te)
dim(test)
dim(tr)
